<!doctype html>
<html>

<head>
  <title>Project Title</title>
  <meta charset="utf-8" name="viewport" content="width=device-width, initial-scale=1">
  <link href="https://use.fontawesome.com/releases/v5.2.0/css/all.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="https://ajax.googleapis.com/ajax/libs/jqueryui/1.12.1/themes/base/jquery-ui.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="css/frame.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="css/controls.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="css/widgets.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="css/custom.css" media="screen" rel="stylesheet" type="text/css" />
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Open+Sans+Condensed:300,700' rel='stylesheet' type='text/css'>
  <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700" rel="stylesheet">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script src="https://ajax.googleapis.com/ajax/libs/jqueryui/1.12.1/jquery-ui.min.js"></script>
  <script src="js/menu.js"></script>
  <script src="js/widgets.js"></script>
  <script src="js/custom.js"></script>
  <style>
    .menu-model {
      color: rgb(255, 255, 255) !important;
      opacity: 1 !important;
      font-weight: 700 !important;
    }
  </style>
</head>

<body>
  <div class="menu-container"></div>
  <div class="content-container">
    <div class="content">
      <div class="content-table flex-column">
        <div class="flex-row">
         
            <p class="text">To build each model, we start with loading the dataset and set the initial image size. Then, we train all models on low resolution images and apply progressive resizing to observer misclassified images. After we build the models, we apply our models to evaluate on the test dataset. Finally, we save and export the models for the ensemble learning model. Before the introduction of ResNet, the training of a very deep neural network was hard due to the problem of notorious vanishing gradients.</p>
  
        </div>

       
          <div class="flex-item flex-column">
            <h2 class="add-top-margin">ResNet50</h2>
            <hr>
            <p class="text">
              RestNet (Residual Networks) is one of the classic backbone neural network models used for computer vision tasks. ResNet offers a deep residual learning framework which significantly eases the training of extremely deep neural networks. Instead of simply stacking the layers of neural networks to increase the network depth, ResNet introduces something called identity shortcut connection by a residual block that skips one or more layers. [source2]
              <br>
              <a href="javascript:void(0)" class="flex-column"><img src="img/resnet_block.png" width=600 height=400>
              </a>
              ResNet seeks to make the approximation of the identity function easier by adding explicit residual connections. In this project, we adopted the version ResNet50 which is a convolutional neural network of 50 layers.
              <br>
              By using ResNet50, we got the accuracy and ROC-AUC rate of 0.9136 and 0.9711 over the testing dataset. Applying test time augmentation, we could even improve the result of both accuracy and ROC-AUC rate to 0.9227 and 0.9756.
              <br>
              The following is a set of sample images of incorrect predictions with top 3 highest loss by ResNet50.
              <a href="javascript:void(0)" class="flex-column"><img src="img/resnet_prediction.png" width=630 height=270>
              </a>
            </p>

            <h2 class="add-top-margin">ResNeXt101</h2>
            <hr>
            <p class="text">
              ResNeXt is a simple, but highly modularized neural network architecture for image classification. The network is constructed by repeating a building block that aggregates a set of transformations with the same topology. [source3] The simplicity of its network design will result in a homogeneous, multi-branch architecture which reduces the number of hyperparameters required by conventional ResNet. This strategy exposes an additional dimension called “cardinality” on top of the depth and width of ResNet. Cardinality is actually the size of the set of transformations. The following two figures demonstrate both conventional ResNet block (left) and the ResNeXt block. The ResNeXt block has cardinality of 32 indicating that the same transformations are being applied 32 times and the result is aggregated in the very end.
              <br>
              <a href="javascript:void(0)" class="flex-column"><img src="img/resnext_block.png" width=630 height=270>
              </a>
              In this project, we adopted the version ResNeXt101. By using ResNeXt101, we got the accuracy and ROC-AUC rate of 0.8788 and 0.9576 over the testing dataset. Applying test time augmentation, we could even improve the result of both accuracy and ROC-AUC rate to 0.9136 and 0.9693. The following is a set of sample images of incorrect predictions with top 3 highest loss by ResNeXt101.
              <a href="javascript:void(0)" class="flex-column"><img src="img/resnext_prediction.png" width=630 height=270>
              </a>
            </p>

            <h2 class="add-top-margin">VGG16</h2>
            <hr>
            <p class="text">
              VGG16 is a convolutional neural network architecture and still considered to be one of the most used vision model architectures now. VGG takes 224x224 pixel RGB images as input. One of the special features of VGG 16 when compared to a standard convolutional network is that the model focuses on having a very small receptive field (3x3 filter) and always using the same padding and max pooling layer of 2x2 filter. The small sized convolution filters allows VGG to have a large number of weight layers, which leads to improved performance. In this project, we adopted the version VGG16.
              <br>
              <a href="javascript:void(0)" class="flex-column"><img src="img/vgg_block.png" width=630 height=270>
              </a>
              By using VGG16, we got the accuracy and ROC-AUC rate of 0.8939 and 0.9676 over the testing dataset. Applying test time augmentation, we could even improve the result of both accuracy and ROC-AUC rate to 0.8970 and 0.9694. The following is a set of sample images of incorrect predictions with top 3 highest loss by VGG16.
              <a href="javascript:void(0)" class="flex-column"><img src="img/vgg_predict.png" width=630 height=270>
              </a>
            </p>

            <h2 class="add-top-margin">SqueezeNet1.1</h2>
            <hr>
            <p class="text">
              SqueezeNet is a small CNN architecture that employs design strategies to reduce the number of parameters, notably with the use of fire modules that "squeeze" parameters using 1x1 convolutions instead of 3x3. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques, the size of the SqueezeNet could be compressed to less than 0.5MB (510 times smaller than AlexNet). [source4] In this project, we adopted SqueezeNet1.1.
              <br>
               <a href="javascript:void(0)" class="flex-column"><img src="img/squeezenet_block.png" width=630 height=270>
              </a>
              By using SqueezeNet1.1, we got the accuracy and ROC-AUC rate of 0.8742 and 0.9538 over the testing dataset. Applying test time augmentation, we could even improve the result of both accuracy and ROC-AUC rate to 0.8939 and 0.9619. The following is a set of sample images of incorrect predictions with top 3 highest loss by SqueezeNet1.1.
               <a href="javascript:void(0)" class="flex-column"><img src="img/squeezenet_predict.png" width=630 height=270>
              </a>
            </p>

            <h2 class="add-top-margin">EfficientNet-B0</h2>
            <hr>
            <p class="text">
              CNN is normally developed at a fixed resource budget and then if more resources are available, it gets scaled up for better accuracy. In precious work, it’s pretty common to scale only one of the three dimensions: the depth of the network, the number of channels, and the input image resolution. Scaling two or even all three dimensions together is possible to deliver better results. The EfficientNet is a CNN architecture which implements a scaling method named compound scaling that strategically scales all three dimensions. In this project we adopted EfficientNet-B0 which is the baseline network developed by AutoML MNAS, while Efficient-B1 to B7 are obtained by scaling up the baseline network. [source5]
              <br>
              By using EfficientNet-B0, we got the accuracy and ROC-AUC rate of 0.9076 and 0.9719 over the testing dataset. Applying test time augmentation, we could even improve the result of both accuracy and ROC-AUC rate to 0.9212 and 0.9742. The following is a set of sample images of incorrect predictions with top 3 highest loss by EfficientNet-B0.
              <a href="javascript:void(0)" class="flex-column"><img src="img/efficientnet_predict.png" width=630 height=270>
              </a>
            </p>

            <h2 class="add-top-margin">DenseNet169</h2>
            <hr>
            <p class="text">
              Convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. For the traditional convolutional networks, due to the longer path between the input layer and the output layer, the information vanishes before reaching its destination. DenseNet connects each layer to every other layer in a feed-forward fashion. [source6] It enjoys several compelling advantages including alleviating the vanishing-gradient problem, strengthening feature propagation, encouraging feature reuse, and substantially reducing the number of parameters.
              <br>
              <a href="javascript:void(0)" class="flex-column"><img src="img/densenet_block.png" width=630 height=270>
              </a>
              In this project, we adopted the version DenseNet169 which specifically has 169 layers in the neural network. By using DenseNet169, we got the accuracy and ROC-AUC rate of 0.9015 and 0.9663 over the testing dataset. Applying test time augmentation, we could even improve the result of accuracy to 0.9061. The following is a set of sample images of incorrect predictions with top 3 highest loss by DenseNet169.
              <a href="javascript:void(0)" class="flex-column"><img src="img/densenet_predict.png" width=630 height=270>
              </a>
            </p>

            <h2 class="add-top-margin">Ensemble Learning (EL)</h2>
            <hr>
            <p class="text">
              We combine all models (ResNet50, VGG16, SqueezeNet, ResNext, DenseNet169, and EfficientNet-B0) to predict and classify skin-cancer images. In our ensemble learning, we have two sections, which are Meta learner (stacking), and Grid search weighted average ensemble [source7]. We use validation images for training ensemble learning models. For Each section, the 10-fold cross-validation is used for measuring EL performance with AUC score. After having the optimal meta classifier and the best set of corresponding weights for transfer learning, we then evaluate the EL performance on a test dataset. In stacking meta learner, there are three subsections which use different classifiers i.e. Logistic Regression, Linear SVC, and Kernel SVC.
              <a href="javascript:void(0)" class="flex-column"><img src="img/ensemble_learning.png" width=630 height=270>
              </a>
            </p>
          </div>
        </div>




        <!-------------------------------------------------------------------------------------------->
        
        <!-------------------------------------------------------------------------------------------->
        <div class="flex-row">
          <div class="flex-item flex-item-stretch flex-column">
            <p class="text text-small text-italic">
              Credits: <span class="highlight-text">Organization One</span>: Author One and Author Two / <span class="highlight-text">Organization Two</span>: Author Three and Author Four
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</body>

</html>
